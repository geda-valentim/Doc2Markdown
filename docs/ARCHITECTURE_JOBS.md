# Arquitetura de Jobs - Doc2MD

## üìä Hierarquia de Jobs

```
MAIN JOB (user request)
  ‚îî‚îÄ> SPLIT JOB (divide PDF)
       ‚îú‚îÄ> PAGE JOB 1 (converte p√°gina 1)
       ‚îú‚îÄ> PAGE JOB 2 (converte p√°gina 2)
       ‚îú‚îÄ> PAGE JOB 3 (converte p√°gina 3)
       ‚îî‚îÄ> ...
       ‚îî‚îÄ> MERGE JOB (combina resultados)
```

## üóÇÔ∏è Estrutura de Dados no Redis

### Main Job
```
job:{main_job_id}:status = {
  "type": "main",
  "status": "processing",
  "progress": 0,
  "created_at": "...",
  "started_at": "...",
  "completed_at": null,
  "error": null,
  "source_type": "file",
  "file_format": "pdf",
  "total_pages": 10,
  "child_jobs": {
    "split_job_id": "split-abc-123",
    "page_jobs": ["page-1-abc", "page-2-abc", ...],
    "merge_job_id": "merge-xyz-789"
  }
}
```

### Split Job
```
job:{split_job_id}:status = {
  "type": "split",
  "status": "completed",
  "parent_job_id": "main-abc-123",
  "total_pages": 10,
  "pages_created": 10,
  "created_at": "...",
  "completed_at": "..."
}
```

### Page Job (cada p√°gina)
```
job:{page_job_id}:status = {
  "type": "page",
  "status": "completed",
  "parent_job_id": "main-abc-123",
  "page_number": 1,
  "page_file_path": "/tmp/doc2md/.../page_0001.pdf",
  "created_at": "...",
  "started_at": "...",
  "completed_at": "...",
  "error": null
}

job:{page_job_id}:result = {
  "markdown": "# Conte√∫do da p√°gina 1...",
  "metadata": {
    "page_number": 1,
    "words": 250,
    "size_bytes": 50000
  }
}
```

### Merge Job
```
job:{merge_job_id}:status = {
  "type": "merge",
  "status": "processing",
  "parent_job_id": "main-abc-123",
  "total_pages": 10,
  "pages_merged": 0,
  "created_at": "...",
  "completed_at": null
}

job:{merge_job_id}:result = {
  "markdown": "# Combined from all pages...",
  "metadata": {
    "pages": 10,
    "total_words": 2500,
    "total_size_bytes": 500000
  }
}
```

## üîÑ Fluxo de Processamento

### 1. Upload (Main Job Created)
```python
POST /convert
  ‚Üí main_job_id = uuid4()
  ‚Üí redis.set_job_status(main_job_id, "queued")
  ‚Üí celery.process_conversion.delay(main_job_id)
  ‚Üí return main_job_id
```

### 2. Process Conversion Task
```python
def process_conversion(main_job_id):
    # Download file
    file_path = download_file()

    if is_pdf_multi_page(file_path):
        # Create SPLIT JOB
        split_job_id = create_split_job(main_job_id)
        celery.split_pdf.delay(split_job_id, file_path)
    else:
        # Convert directly
        result = convert_to_markdown(file_path)
        store_result(main_job_id, result)
```

### 3. Split PDF Task
```python
def split_pdf(split_job_id):
    parent_job = get_parent_job(split_job_id)

    # Split into pages
    page_files = pdf_splitter.split(file_path)

    # Create PAGE JOBS
    page_job_ids = []
    for page_num, page_file in page_files:
        page_job_id = create_page_job(parent_job.id, page_num, page_file)
        page_job_ids.append(page_job_id)

        # Launch page conversion
        celery.convert_page.delay(page_job_id)

    # Update parent job
    update_child_jobs(parent_job.id, page_jobs=page_job_ids)

    # Mark split as completed
    complete_job(split_job_id)
```

### 4. Convert Page Task
```python
def convert_page(page_job_id):
    page_job = get_job(page_job_id)

    # Convert page
    result = docling.convert(page_job.page_file_path)

    # Store page result
    store_result(page_job_id, result)

    # Mark page as completed
    complete_job(page_job_id)

    # Check if all pages completed
    parent_job = get_parent_job(page_job_id)
    if all_pages_completed(parent_job.id):
        # Create MERGE JOB
        merge_job_id = create_merge_job(parent_job.id)
        celery.merge_pages.delay(merge_job_id)
```

### 5. Merge Pages Task
```python
def merge_pages(merge_job_id):
    parent_job = get_parent_job(merge_job_id)
    page_jobs = get_all_page_jobs(parent_job.id)

    # Combine all pages in order
    combined_markdown = ""
    for page_job in sorted(page_jobs, key=lambda x: x.page_number):
        page_result = get_result(page_job.id)
        combined_markdown += page_result.markdown + "\n\n---\n\n"

    # Store merged result
    store_result(merge_job_id, combined_markdown)

    # Mark merge as completed
    complete_job(merge_job_id)

    # Mark parent as completed
    complete_job(parent_job.id)
```

## üì° Endpoints Atualizados

### GET /jobs/{job_id}
Retorna status do job (qualquer tipo)

```json
// Main Job
{
  "job_id": "main-abc-123",
  "type": "main",
  "status": "processing",
  "progress": 40,
  "total_pages": 10,
  "pages_completed": 4,
  "child_jobs": {
    "split_job_id": "split-abc-123",
    "page_jobs": ["page-1-abc", "page-2-abc", ...],
    "merge_job_id": null
  },
  "created_at": "...",
  "started_at": "...",
  "completed_at": null
}

// Page Job
{
  "job_id": "page-1-abc",
  "type": "page",
  "status": "completed",
  "parent_job_id": "main-abc-123",
  "page_number": 1,
  "created_at": "...",
  "completed_at": "..."
}
```

### GET /jobs/{job_id}/result
Retorna resultado do job

```json
// Main Job Result (ap√≥s merge)
{
  "job_id": "main-abc-123",
  "type": "main",
  "result": {
    "markdown": "# Combined...",
    "metadata": {
      "pages": 10,
      "total_words": 2500
    }
  }
}

// Page Job Result (p√°gina individual) ‚≠ê NOVO
{
  "job_id": "page-1-abc",
  "type": "page",
  "page_number": 1,
  "result": {
    "markdown": "# P√°gina 1...",
    "metadata": {
      "page_number": 1,
      "words": 250
    }
  }
}
```

### GET /jobs/{main_job_id}/pages
Lista todas as p√°ginas do job principal

```json
{
  "job_id": "main-abc-123",
  "total_pages": 10,
  "pages": [
    {
      "page_number": 1,
      "job_id": "page-1-abc",
      "status": "completed",
      "url": "/jobs/page-1-abc/result"
    },
    {
      "page_number": 2,
      "job_id": "page-2-abc",
      "status": "processing",
      "url": "/jobs/page-2-abc/result"
    }
  ]
}
```

### GET /jobs/{page_job_id}/result ‚≠ê NOVO
Consulta resultado de UMA p√°gina espec√≠fica

```bash
# Exemplo
curl http://localhost:8080/jobs/page-1-abc/result

# Response
{
  "job_id": "page-1-abc",
  "type": "page",
  "page_number": 1,
  "parent_job_id": "main-abc-123",
  "status": "completed",
  "result": {
    "markdown": "# Conte√∫do da p√°gina 1...",
    "metadata": {
      "page_number": 1,
      "words": 250,
      "size_bytes": 50000
    }
  },
  "completed_at": "2025-10-01T18:30:00Z"
}
```

## üéØ Casos de Uso

### Caso 1: Consultar Progresso Geral
```bash
GET /jobs/main-abc-123
‚Üí "progress": 40, "pages_completed": 4/10
```

### Caso 2: Ver Todas as P√°ginas
```bash
GET /jobs/main-abc-123/pages
‚Üí Lista de 10 p√°ginas com seus job_ids
```

### Caso 3: Ver P√°gina Espec√≠fica (Individual)
```bash
GET /jobs/page-5-abc/result
‚Üí Markdown da p√°gina 5 isoladamente
```

### Caso 4: Ver Resultado Completo (Merged)
```bash
GET /jobs/main-abc-123/result
‚Üí Markdown de todas p√°ginas combinadas
```

## üîë Vantagens da Nova Arquitetura

### 1. **Rastreabilidade Completa**
- Cada opera√ß√£o tem seu pr√≥prio job_id
- F√°cil debug e monitoramento
- Hist√≥rico completo do processamento

### 2. **Consulta Granular**
- Pode consultar p√°gina individual: `/jobs/page-3-abc/result`
- Pode consultar resultado final: `/jobs/main-abc-123/result`
- Pode consultar status de split/merge

### 3. **Retry Inteligente**
- Se p√°gina 5 falhar, retry apenas dela
- Se merge falhar, retry apenas merge (p√°ginas j√° processadas)
- N√£o precisa refazer split se j√° foi feito

### 4. **Flexibilidade**
- Frontend pode mostrar p√°ginas individualmente
- Pode fazer download parcial (s√≥ p√°gina X)
- Pode refazer apenas parte do processo

### 5. **Escalabilidade**
- Cada tipo de job pode ter workers dedicados
- Fila separada para split, pages, merge
- Prioriza√ß√£o por tipo de opera√ß√£o

## üìä Tipos de Jobs

```python
class JobType(str, Enum):
    MAIN = "main"          # Job principal do usu√°rio
    SPLIT = "split"        # Divis√£o de PDF em p√°ginas
    PAGE = "page"          # Convers√£o de p√°gina individual
    MERGE = "merge"        # Combina√ß√£o de p√°ginas
    DOWNLOAD = "download"  # Download de URL/GDrive/Dropbox (opcional)
```

## üîÑ Estados dos Jobs

```python
class JobStatus(str, Enum):
    QUEUED = "queued"         # Na fila
    PROCESSING = "processing" # Sendo processado
    COMPLETED = "completed"   # Conclu√≠do com sucesso
    FAILED = "failed"         # Falhou
    CANCELLED = "cancelled"   # Cancelado pelo usu√°rio
```

## üìà C√°lculo de Progress

### Main Job Progress
```python
def calculate_main_job_progress(main_job_id):
    page_jobs = get_all_page_jobs(main_job_id)
    total = len(page_jobs)
    completed = sum(1 for pj in page_jobs if pj.status == "completed")

    # Pesos:
    # Split: 10%
    # Pages: 80% (distribu√≠do entre p√°ginas)
    # Merge: 10%

    split_done = 10 if split_job.status == "completed" else 0
    pages_done = int((completed / total) * 80)
    merge_done = 10 if merge_job.status == "completed" else 0

    return split_done + pages_done + merge_done
```

## üóÉÔ∏è Schema Unificado

```python
class Job(BaseModel):
    job_id: UUID
    type: JobType
    status: JobStatus
    parent_job_id: Optional[UUID] = None
    progress: int = 0
    created_at: datetime
    started_at: Optional[datetime] = None
    completed_at: Optional[datetime] = None
    error: Optional[str] = None

    # Type-specific fields
    page_number: Optional[int] = None         # For PAGE jobs
    total_pages: Optional[int] = None          # For MAIN/SPLIT jobs
    child_job_ids: Optional[List[UUID]] = None # For MAIN jobs
```

## üöÄ Implementa√ß√£o

### Prioridade de Tasks

1. ‚úÖ Atualizar schemas com JobType
2. ‚úÖ Criar task `split_pdf_task`
3. ‚úÖ Criar task `merge_pages_task`
4. ‚úÖ Atualizar task `process_page` para job_id pr√≥prio
5. ‚úÖ Atualizar Redis client para hierarquia
6. ‚úÖ Atualizar endpoints para suportar diferentes tipos
7. ‚úÖ Adicionar endpoint para p√°gina individual

Esta arquitetura d√° **visibilidade completa** e **controle granular** de todo o processo! üéØ
